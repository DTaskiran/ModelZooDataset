{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "This notebook contains examples of loading and exploring preprocessed and raw model zoos to the custom dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "\n",
    "# set environment variables to limit cpu usage\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\"  # export OMP_NUM_THREADS=4\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"4\"  # export OPENBLAS_NUM_THREADS=4\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"6\"  # export MKL_NUM_THREADS=6\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"4\"  # export VECLIB_MAXIMUM_THREADS=4\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"6\"  # export NUMEXPR_NUM_THREADS=6\n",
    "from checkpoints_to_datasets.dataset_base import ModelDatasetBase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dspath = Path(\n",
    "    'path/to/dataset_cifar_small_hyp_fix.pt')\n",
    "ds = torch.load(dspath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset contains a \"trainset\", \"valset\" and \"testset\"\n",
    "print(ds.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the weights can be accessed by calling dataset.__get_weights__()\n",
    "weights_test = ds['testset'].__get_weights__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model properties are contained in a 'properties' dictionary\n",
    "print(ds['testset'].properties.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_from_path(path:Path,dstype:str,dssize:str=\"small\",epoch_list:list=[5,15,25]):\n",
    "    \"\"\"\n",
    "    Loads custom dataset class from raw zoo.\n",
    "    input path: pathlib.Path to raw model zoo.\n",
    "    input dstype: str \"Seed\",\"Hyp-fix\" or \"Hyp-rand\" setting the dataset type\n",
    "    input dssize: str \"small\" or \"large\" depending on the CNN size in the zoo\n",
    "    input epoch_lst: list of integers, indicating the epochs of which to load the models \n",
    "    return dataset: dict with \"trainset\", \"valset\", \"testset\" \n",
    "    \"\"\"\n",
    "    # compose properties to map for\n",
    "    result_key_list = [\n",
    "        \"test_acc\",\n",
    "        \"training_iteration\",\n",
    "        \"ggap\",\n",
    "    ]\n",
    "    if dstype == \"Seed\":\n",
    "        config_key_list = []\n",
    "    else:\n",
    "        config_key_list = [\n",
    "            \"model::nlin\", \n",
    "            \"model::init_type\",\n",
    "            \"optim::optimizer\",\n",
    "            \"model::dropout\",\n",
    "            \"optim::lr\",\n",
    "            \"optim::wd\"\n",
    "        ]\n",
    "    property_keys = {\n",
    "        \"result_keys\": result_key_list,\n",
    "        \"config_keys\": config_key_list,\n",
    "    }\n",
    "\n",
    "    ## set layer list. Large model zoos require the first, small zoos the second version.\n",
    "    if dssize==\"large\":\n",
    "        layer_lst = [\n",
    "            (0, \"conv2d\"),\n",
    "            (4, \"conv2d\"),\n",
    "            (8, \"conv2d\"),\n",
    "            (13, \"fc\"),\n",
    "            (16, \"fc\"),\n",
    "        ]\n",
    "    else:\n",
    "        layer_lst = [\n",
    "            (0, \"conv2d\"),\n",
    "            (3, \"conv2d\"),\n",
    "            (6, \"conv2d\"),\n",
    "            (9, \"fc\"),\n",
    "            (11, \"fc\"),\n",
    "        ]\n",
    "    \n",
    "    # initialize ray\n",
    "    import ray\n",
    "    if ray.is_initialized():\n",
    "        ray.shutdown()\n",
    "    \n",
    "    # set dataset path\n",
    "    path_zoo_root = [path.absolute()]\n",
    "        \n",
    "    # load datasets\n",
    "    # trainset\n",
    "    trainset = ModelDatasetBase(\n",
    "            root=path_zoo_root,\n",
    "            layer_lst=layer_lst,\n",
    "            epoch_lst=epoch_lst,\n",
    "            mode=\"checkpoint\",\n",
    "            task=\"reconstruction\",  # \"reconstruction\" (x->x), \"sequence_prediction\" (x^i -> x^i+1),\n",
    "            use_bias=True,\n",
    "            train_val_test=\"train\",  # determines whcih dataset split to use\n",
    "            ds_split=[0.7, 0.15, 0.15],  #\n",
    "            max_samples=None,\n",
    "            weight_threshold=5,\n",
    "            filter_function=None,  # gets sample path as argument and returns True if model needs to be filtered out\n",
    "            property_keys=property_keys,\n",
    "            num_threads=6,\n",
    "            verbosity=0,\n",
    "            shuffle_path=True,\n",
    "    )\n",
    "    # valset\n",
    "    valset = ModelDatasetBase(\n",
    "            root=path_zoo_root,\n",
    "            layer_lst=layer_lst,\n",
    "            epoch_lst=epoch_lst,\n",
    "            mode=\"checkpoint\",\n",
    "            task=\"reconstruction\",  # \"reconstruction\" (x->x), \"sequence_prediction\" (x^i -> x^i+1),\n",
    "            use_bias=True,\n",
    "            train_val_test=\"val\",  # determines whcih dataset split to use\n",
    "            ds_split=[0.7, 0.15, 0.15],  #\n",
    "            max_samples=None,\n",
    "            weight_threshold=5,\n",
    "            filter_function=None,  # gets sample path as argument and returns True if model needs to be filtered out\n",
    "            property_keys=property_keys,\n",
    "            num_threads=6,\n",
    "            verbosity=0,\n",
    "            shuffle_path=True,\n",
    "    )\n",
    "    # testset\n",
    "    testset = ModelDatasetBase(\n",
    "            root=path_zoo_root,\n",
    "            layer_lst=layer_lst,\n",
    "            epoch_lst=epoch_lst,\n",
    "            mode=\"checkpoint\",\n",
    "            task=\"reconstruction\",  # \"reconstruction\" (x->x), \"sequence_prediction\" (x^i -> x^i+1),\n",
    "            use_bias=True,\n",
    "            train_val_test=\"test\",  # determines whcih dataset split to use\n",
    "            ds_split=[0.7, 0.15, 0.15],  #\n",
    "            max_samples=None,\n",
    "            weight_threshold=5,\n",
    "            filter_function=None,  # gets sample path as argument and returns True if model needs to be filtered out\n",
    "            property_keys=property_keys,\n",
    "            num_threads=6,\n",
    "            verbosity=0,\n",
    "            shuffle_path=True,\n",
    "    )\n",
    "    # put in dictionary\n",
    "    dataset = {\n",
    "        \"trainset\": trainset,\n",
    "        \"valset\": valset,\n",
    "        \"testset\": testset,\n",
    "    }\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dspath_raw = Path(\n",
    "    '/path/to/tune_zoo_mnist_uniform')\n",
    "dstype=\"Seed\"\n",
    "dssize=\"small\"\n",
    "# set list of epochs to load\n",
    "epoch_lst = [5,15,50]\n",
    "# epoch_lst = list(range(0,51))\n",
    "\n",
    "ds_custom = load_dataset_from_path(path=dspath_raw,dstype=dstype,dssize=dssize,epoch_list=epoch_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c94a9afd5f305ac053d2b5055fd087fc551fd4a4f3bde3748163f2f67a39aea"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('neurips_hyper_reps')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
